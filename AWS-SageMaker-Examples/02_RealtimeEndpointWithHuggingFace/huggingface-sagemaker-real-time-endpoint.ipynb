{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049; text-align: center; border-radius: 5px 5px; padding: 5px\"> Bring your BERT Model to AWS SageMaker with Script Mode </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"img/BringYourOwnModel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my [previous post](https://medium.com/analytics-vidhya/aws-sagemaker-train-deploy-and-update-a-hugging-face-bert-model-eeefc8211368) I have discussed about how to train a Hugging Face BERT model using on-demand and spot instances, deploy that fine-tuned model on SageMaker real-time endpoint and update that endpoint as well. This makes our work easier because SageMaker supports managed training and inference for a variety of [ML frameworks](https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html) such as XGBoost, TensorFlow, PyTorch and HuggingFace etc.\n",
    "\n",
    "Now let's discuss the use cases of using SageMaker [script mode](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-script-mode/index.html):\n",
    "\n",
    "1. When we trained a model outside of SageMaker (might have trained on local Jupyter notebook, Google colab, AWS EC2 instances and SageMaker notebook instance etc) then we can bring our fine-tuned model with custom inference script, dependent libraries (can be specified in requirements.txt file) and deploy it on SageMaker endpoints for inference.\n",
    "\n",
    "2. If we want to train models using a custom algorithm not supported by one of the [built-in algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html) or we want to train a model with custom training script then we can make use of script mode.\n",
    "\n",
    "This notebook will demonstrate how to host fine-tuned BERT model with custom inference script on SageMaker real-time endpoint.\n",
    "\n",
    "Please refer to [Medium article](https://medium.com/@vinayakshanawad/bring-your-own-model-with-amazon-sagemaker-script-mode-6cf374747f9e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Development Environment and Permissions </h2>\n",
    "\n",
    "NOTE: You can run this demo in Sagemaker Studio, your local machine, or Sagemaker Notebook Instances\n",
    "\n",
    "If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'huggingface-bert-model-deploy'\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Store Model Artifacts </h2>\n",
    "\n",
    "As discussed in my previous post, I've taken dataset from [Kaggle competition](https://www.kaggle.com/c/nlp-getting-started/overview) which consists of fake and real Tweets about disasters. The task is to classify the tweets.\n",
    "\n",
    "Trained a HuggingFace BERT model using on-demand instances with hyperparameter (epoch = 2) value and we can store our fine-tuned [BERT](https://arxiv.org/abs/1810.04805) model artifacts in `model/` directory and define `model_path`.\n",
    "\n",
    "**Note**: I've used fine-tuned BERT model using on-demand instances to show you how we can bring bring our own model with SageMaker script mode, but in real-life scenario we can bring in our own models which are trained outside of SageMaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.json', 'model.tar.gz', '.ipynb_checkpoints', 'pytorch_model.bin']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path = 'model'\n",
    "os.listdir(os.path.join(\"/home/ec2-user/SageMaker/\", model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Write the Inference Script </h2> \n",
    "\n",
    "Since we are bringing our own model to SageMaker, we must create an inference script. The script will run inside our HuggingFace container. Our script should include a function for model loading, and optionally functions generating predictions, and input/output processing. The HuggingFace container provides default implementations for generating a prediction and input/output processing. By including these functions in your script you are overriding the default functions. You can find additional [details here](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#serve-a-pytorch-model).\n",
    "\n",
    "**Note**:\n",
    "\n",
    "To install additional libraries at container startup, we can add a requirements.txt text file that specifies the libraries to be installed using pip. Within the archive, the HuggingFace container expects all inference code and requirements.txt file to be inside the code/ directory.\n",
    "\n",
    "In the next cell we'll see our inference script for BERT model which helps us to predict whether tweet is real disaster or not. \n",
    "\n",
    "You will notice that it uses the [transformers library from Hugging Face](https://huggingface.co/docs/transformers/index) and installed using pip command in inference script, likewise we need to install additional libraries if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {model_path}/code\n",
    "\n",
    "! cp code/inference.py {model_path}/code/inference.py\n",
    "! cp code/requirements.txt {model_path}/code/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BertForSequenceClassification, BertTokenizer\n",
      "\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
      "\n",
      "MAX_LEN = \u001b[34m64\u001b[39;49;00m  \u001b[37m# this is the max length of the sentence\u001b[39;49;00m\n",
      "\n",
      "\u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading BERT tokenizer...\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "tokenizer = BertTokenizer.from_pretrained(\u001b[33m\"\u001b[39;49;00m\u001b[33mbert-base-uncased\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, do_lower_case=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model = BertForSequenceClassification.from_pretrained(model_dir)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ model loaded ===========================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    model.to(device)\n",
      "    model.eval()\n",
      "    \n",
      "    class_label = {\u001b[34m1\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mReal disaster\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "               \u001b[34m0\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mNot a disaster\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}\n",
      "\n",
      "    input_id, input_mask = input_data\n",
      "    input_id = input_id.to(device)\n",
      "    input_mask = input_mask.to(device)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m============== encoded data =================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[36mprint\u001b[39;49;00m(input_id, input_mask)\n",
      "    \u001b[34mwith\u001b[39;49;00m torch.no_grad():\n",
      "        y = model(input_id, attention_mask=input_mask)[\u001b[34m0\u001b[39;49;00m]\n",
      "        result = \u001b[36mlist\u001b[39;49;00m(np.argmax(y, axis=\u001b[34m1\u001b[39;49;00m))\n",
      "        result = [\u001b[36mint\u001b[39;49;00m(l) \u001b[34mfor\u001b[39;49;00m l \u001b[35min\u001b[39;49;00m result]\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=============== inference result =================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        predicted_labels = [class_label[l] \u001b[34mfor\u001b[39;49;00m l \u001b[35min\u001b[39;49;00m result]\n",
      "        \u001b[36mprint\u001b[39;49;00m(predicted_labels)\n",
      "    \u001b[34mreturn\u001b[39;49;00m predicted_labels\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(request_body, request_content_type):\n",
      "    \u001b[33m\"\"\"An input_fn that loads a pickled tensor\"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m request_content_type == \u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        data = json.loads(request_body)\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ input sentences ===============\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(data)\n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data, \u001b[36mstr\u001b[39;49;00m):\n",
      "            data = [data]\n",
      "        \u001b[34melif\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data, \u001b[36mlist\u001b[39;49;00m) \u001b[35mand\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(data) > \u001b[34m0\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[36misinstance\u001b[39;49;00m(data[\u001b[34m0\u001b[39;49;00m], \u001b[36mstr\u001b[39;49;00m):\n",
      "            \u001b[34mpass\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUnsupported input type. Input type can be a string or an non-empty list. \u001b[39;49;00m\u001b[33m\\\u001b[39;49;00m\n",
      "\u001b[33m                             I got \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(data))\n",
      "                       \n",
      "        input_ids = [tokenizer.encode(x, add_special_tokens=\u001b[34mTrue\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m data]\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================ encoded sentences ==============\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(input_ids)\n",
      "\n",
      "        \u001b[37m# pad shorter sentence\u001b[39;49;00m\n",
      "        padded =  torch.zeros(\u001b[36mlen\u001b[39;49;00m(input_ids), MAX_LEN) \n",
      "        \u001b[34mfor\u001b[39;49;00m i, p \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(input_ids):\n",
      "            padded[i, :\u001b[36mlen\u001b[39;49;00m(p)] = torch.tensor(p)\n",
      "     \n",
      "        \u001b[37m# create mask\u001b[39;49;00m\n",
      "        mask = (padded != \u001b[34m0\u001b[39;49;00m)\n",
      "        \n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m================= padded input and attention mask ================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[36mprint\u001b[39;49;00m(padded, \u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, mask)\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m padded.long(), mask.long()\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUnsupported content type: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(request_content_type))\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction, response_content_type):\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m    Serialize and prepare the prediction output\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m response_content_type == \u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        response = json.dumps(prediction)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        response = json.dumps(prediction)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m response\n"
     ]
    }
   ],
   "source": [
    "!pygmentize {model_path}/code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Package Model </h2> \n",
    "\n",
    "For hosting, SageMaker requires that the deployment package be structured in a compatible format. It expects all files to be packaged in a tar archive named \"model.tar.gz\" with gzip compression. Within the archive, the HuggingFace container expects all inference code file to be inside the `code/` directory. See the guide here for a thorough explanation of the required directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./config.json\n",
      "./model.tar.gz\n",
      "./.ipynb_checkpoints/\n",
      "./code/\n",
      "./code/inference.py\n",
      "./code/requirements.txt\n",
      "./pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!tar -czvf {model_path}/model.tar.gz -C {model_path}/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Upload HuggingFace model to S3 </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "model_data = S3Uploader.upload('model/model.tar.gz', 's3://{0}/{1}/models'.format(bucket,prefix))\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Create SageMaker Real-time Endpoint </h2> \n",
    "\n",
    "After we upload BERT model to S3 we can deploy our endpoint. To create/deploy a real-time endpoint with boto3 you need to create a \"SageMaker Model\", a \"SageMaker Endpoint Configuration\" and a \"SageMaker Endpoint\". The \"SageMaker Model\" contains our model configuration including our S3 path where we upload/deploy huggingface model. The \"SageMaker Endpoint Configuration\" contains the configuration for the endpoint. The \"SageMaker Endpoint\" is the actual endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SageMaker Model\n",
    "image_uri = \"763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-cpu-py38-ubuntu20.04\"\n",
    "deployment_name = \"huggingface-bert-model\"\n",
    "\n",
    "primary_container = {\n",
    "    'Image': image_uri,\n",
    "    'ModelDataUrl': model_data,\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "        'SAGEMAKER_REGION': region,\n",
    "        'SAGEMAKER_SUBMIT_DIRECTORY': model_data\n",
    "    }\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(ModelName = f\"{deployment_name}-v1\",\n",
    "                                              ExecutionRoleArn = get_execution_role(),\n",
    "                                              PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])\n",
    "\n",
    "# create SageMaker Endpoint configuration\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = f\"{deployment_name}-epc\",\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "        'InstanceType':'ml.m5.4xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName': f\"{deployment_name}-v1\",\n",
    "        'VariantName':'AllTraffic',\n",
    "        'InitialVariantWeight':1\n",
    "        }\n",
    "    ])\n",
    "\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))\n",
    "\n",
    "# create SageMaker Endpoint\n",
    "endpoint_params = {\n",
    "    'EndpointName': f\"{deployment_name}-ep\", 'EndpointConfigName': f\"{deployment_name}-epc\"}\n",
    "endpoint_response = sm_client.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Get Predictions </h2> \n",
    "\n",
    "Now that our API endpoint is deployed, we can send it text to get predictions from our BERT model. You can use the SageMaker SDK or the SageMaker Runtime API to invoke the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: ['Not a disaster', 'Real disaster', 'Real disaster']\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFacePredictor\n",
    "\n",
    "predictor = HuggingFacePredictor(f\"{deployment_name}-ep\")\n",
    "\n",
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.JSONDeserializer()\n",
    "\n",
    "test_sentences = [\"I met my friend today by accident\",\n",
    "                  \"Frank had a severe head injury after the car accident last month\", \n",
    "                  \"Just happened a terrible car crash\"\n",
    "                  ]\n",
    "\n",
    "result = predictor.predict(test_sentences)\n",
    "print(\"result:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Update a SageMaker Real-time Endpoint </h2>\n",
    "\n",
    "As we know Machine Learning is a highly iterative process. During the course of a single project, data scientists and ML engineers routinely train thousands of different models in search of maximum accuracy and other metrics. Indeed, the number of combinations for algorithms, data sets, and training parameters (aka hyperparameters) is infinite.\n",
    "\n",
    "For example, I've considered a trained BERT model with updated hyperparameter (epoch = 3) value and see if model performance is improved or not.\n",
    "\n",
    "If we find the improvement in model performance then we can update the existing SageMaker model endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./config.json\n",
      "./model.tar.gz\n",
      "./.ipynb_checkpoints/\n",
      "./code/\n",
      "./code/inference.py\n",
      "./code/requirements.txt\n",
      "./pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# replace the existing model artifacts (model trained with epoch = 2) with updated artifacts (model trained with epoch = 3) and package it\n",
    "!tar -czvf {model_path}/model.tar.gz -C {model_path}/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload updated model artifacts to S3\n",
    "model_data = S3Uploader.upload('model/model.tar.gz', 's3://{0}/{1}/models'.format(bucket,prefix))\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "# Create SageMaker model\n",
    "deployment_name = \"huggingface-bert-model-v2\"\n",
    "\n",
    "primary_container = {\n",
    "    'Image': image_uri,\n",
    "    'ModelDataUrl': model_data,\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "        'SAGEMAKER_REGION': region,\n",
    "        'SAGEMAKER_SUBMIT_DIRECTORY': model_data\n",
    "    }\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "   ModelName = deployment_name, ExecutionRoleArn = role, PrimaryContainer = primary_container\n",
    ")\n",
    "\n",
    "# update SageMaker Endpoint\n",
    "predictor.update_endpoint(initial_instance_count=1, instance_type=\"ml.m5.4xlarge\", model_name=deployment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Delete the Real-time Endpoint </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Conclusion </h2> \n",
    "\n",
    "In this post, we discussed the use cases for using script mode, and how script mode can accelerate the model deployment process. We successfully deployed our own fine-tuned Hugging Face Transformer model to Amazon SageMaker for inference using the Real-time Endpoint. Real-time endpoints are a great option for inference workloads especially when we have real-time, interactive, low latency requirements. Real-time endpoints are fully managed and support autoscaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading! If you have any questions, feel free to contact me."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
