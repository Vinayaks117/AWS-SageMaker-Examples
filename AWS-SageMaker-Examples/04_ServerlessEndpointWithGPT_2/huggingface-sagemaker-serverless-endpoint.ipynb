{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e395aa",
   "metadata": {},
   "source": [
    "<h3 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049; text-align: center; border-radius: 5px 5px; padding: 5px\"> Pay as you use SageMaker Serverless inference with GPT-2 </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7fbe35",
   "metadata": {},
   "source": [
    "<img src = \"img/Serverless.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f57b1",
   "metadata": {},
   "source": [
    "Huge credits üëè to AWS team for making SageMaker Serverless inference option generally available.\n",
    "\n",
    "Lately, I've been looking for hosting Machine Learning models on serverless infrastructure and found that there are multiple ways in which we can achieve that.\n",
    "1. Using [Serverless framework](https://www.serverless.com/framework/docs/getting-started)\n",
    "\n",
    "    Two options:\n",
    "    * Create a Lambda layer (which contains dependency libraries) and attach it to Lambda function.\n",
    "    * Using Docker container (for example; host Hugging Face BERT models, Image Classification models on S3 and serve it through serverless framework and Lambda functions)\n",
    "2. Using [AWS CDK](https://aws.amazon.com/blogs/compute/hosting-hugging-face-models-on-aws-lambda/) (Cloud Development Kit)\n",
    "3. Using [AWS SAM](https://aws.amazon.com/serverless/sam/) (Serverless Application Model)\n",
    "\n",
    "    Host Deep Learning models on S3, load it on to EFS (like storing models on cache) and serve the inference requests.\n",
    "\n",
    "    Two options:\n",
    "    * [Using SAM Helloworld template](https://towardsdatascience.com/deploying-sklearn-machine-learning-on-aws-lambda-with-sam-8cc69ee04f47) - Create a Lambda function with code and API gateway trigger.\n",
    "    * Using SAM Machine Learning template - Create a docker container with all code then attach it to Lambda function and create an API gateway trigger.\n",
    "4. Using [SageMaker Serverless inference](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html)\n",
    "\n",
    "    The problem with the first three options is that we have to build, manage, and maintain all your containers.\n",
    "    SageMaker (SM) Serverless inference option allows you to focus on the model building process without having to manage the underlying infrastructure. You can choose either a SM in-built container or bring your own.\n",
    "\n",
    "**SageMaker Serverless inference Use¬†cases**\n",
    "\n",
    "Use this option when you don't often receive inference requests the entire day, such as customer feedback service or chatbot applications or analyze data from documents and tolerate cold start problems.\n",
    "Serverless endpoints automatically launch compute resources and scale them in and out based on the workload. You can pay only for invocations and save a lot of cost.\n",
    "\n",
    "**Warming up the Cold¬†Starts**\n",
    "\n",
    "You can create a health-check service to load the model but do not use the model and you can invoke that service periodically or when users are still exploring the application.\n",
    "Use the AWS CloudWatch to keep our lambda service warm.\n",
    "\n",
    "This article will demonstrate how to host pretrained transformers models: GPT-2 model on SageMaker Serverless endpoint using SageMaker boto3 API.\n",
    "\n",
    "NOTE: At the time of writing this only CPU Instances are supported for Serverless Endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0651c5",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Import necessary libraries and Setup permissions </h2>\n",
    "\n",
    "NOTE: You can run this demo in Sagemaker Studio, your local machine, or Sagemaker Notebook Instances\n",
    "\n",
    "If you are going to use Sagemaker in a local environment (not SageMaker Studio or Notebook Instances). You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52af6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'gpt-serverless-model'\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86de44b",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Retrieve Model Artifacts </h2>\n",
    "\n",
    "#### `GPT-2 model`\n",
    "\n",
    "We will download the model artifacts for the pretrained [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model. GPT-2 is a popular text generation model that was developed by OpenAI. Given a text prompt it can generate synthetic text that may follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e6f8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.17.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2278a090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/vocab.json', 'model/merges.txt')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "model_path = 'model/'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "    \n",
    "model.save_pretrained(save_directory=model_path)\n",
    "tokenizer.save_vocabulary(save_directory=model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d82b7e5",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Write the Inference Script </h2> \n",
    "\n",
    "#### `GPT-2 model`\n",
    "\n",
    "In the next cell we'll see our inference script for GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b13e8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir model/code\n",
    "\n",
    "! cp code/inference.py model/code/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4868bb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GPT2Tokenizer, TextGenerationPipeline, GPT2LMHeadModel\n",
      "\n",
      "\u001b[37m# Load the model for inference\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "\n",
      "    \u001b[37m# Load GPT2 tokenizer from disk.\u001b[39;49;00m\n",
      "    vocab_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mvocab.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    merges_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmerges.txt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    tokenizer = GPT2Tokenizer(vocab_file=vocab_path, merges_file=merges_path)\n",
      "\n",
      "    \u001b[37m# Load GPT2 model from disk.\u001b[39;49;00m\n",
      "    model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
      "    \u001b[34mreturn\u001b[39;49;00m TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
      "\n",
      "\u001b[37m# Apply model to the incoming request\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.\u001b[32m__call__\u001b[39;49;00m(input_data)\n",
      "\n",
      "\u001b[37m# Deserialize and prepare the prediction input\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(request_body, request_content_type):\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m request_content_type == \u001b[33m\"\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "        request = json.loads(request_body)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        request = request_body\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m request\n",
      "\n",
      "\u001b[37m# Serialize and prepare the prediction output\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction, response_content_type):\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize model/code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7cfaa",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Package Model </h2> \n",
    "\n",
    "For hosting, SageMaker requires that the deployment package be structed in a compatible format. It expects all files to be packaged in a tar archive named \"model.tar.gz\" with gzip compression. Within the archive, the Hugging Face container expects all inference code files to be inside the `code/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "487ba1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./merges.txt\n",
      "./vocab.json\n",
      "./config.json\n",
      "./.ipynb_checkpoints/\n",
      "./code/\n",
      "./code/inference.py\n",
      "./pytorch_model.bin\n",
      "tar: .: file changed as we read it\n"
     ]
    }
   ],
   "source": [
    "!tar -czvf model/model.tar.gz -C model/ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888e461a",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Upload GPT-2 model to¬†S3 </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe11efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "model_data = S3Uploader.upload('model/model.tar.gz', 's3://{0}/{1}'.format(bucket,prefix))\n",
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513a91e1",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Create and Deploy a Serverless GPT-2¬†model </h2> \n",
    "\n",
    "We are using a CPU based Hugging Face container image to host the inference script, GPUs are not supported in Serverless endpoints and hopefully the AWS team will add GPUs to Serverless endpoints soon üòÑ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "933a1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = \"763104351884.dkr.ecr.us-east-2.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-cpu-py38-ubuntu20.04\"\n",
    "\n",
    "model_name    = 'gpt-2-serverless-model'\n",
    "epc_name     = 'gpt-2-serverless-model-epc'\n",
    "endpoint_name = 'gpt-2-serverless-model-ep'\n",
    "\n",
    "primary_container = {\n",
    "    'Image': image_uri,\n",
    "    'ModelDataUrl': model_data,\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "        'SAGEMAKER_REGION': region,\n",
    "        'SAGEMAKER_SUBMIT_DIRECTORY': model_data\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ae9bb",
   "metadata": {},
   "source": [
    "Next we will create a SageMaker model, endpoint config and endpoint. We have to specify \"ServerlessConfig\" which contains two parameters MemorySizeInMB and MaxConcurrency while creating endpoint config. This is the only difference we have in Serverless endpoint otherwise everything remains same as we do in Real-time inference.\n",
    "\n",
    "MemorySizeInMB: 1024 MB, 2048 MB, 3072 MB, 4096 MB, 5120 MB, or 6144 MB. The memory size should be at least as large as your model size.\n",
    "\n",
    "MaxConcurrency: The maximum number of concurrent invocations your serverless endpoint can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ebb5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Register a GPT-2 model in SM\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "create_model_response = sm_client.create_model(ModelName = model_name,\n",
    "                                              ExecutionRoleArn = get_execution_role(),\n",
    "                                              PrimaryContainer = primary_container)\n",
    "\n",
    "print(create_model_response['ModelArn'])\n",
    "\n",
    "# Create a SM Serverless endpoint config\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = epc_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "        'ServerlessConfig':{\n",
    "            'MemorySizeInMB' : 6144,\n",
    "            'MaxConcurrency' : 5\n",
    "        },\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic',\n",
    "        'InitialVariantWeight':1\n",
    "        }\n",
    "    ])\n",
    "\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))\n",
    "\n",
    "# Create a SM Serverless endpoint config\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': epc_name,\n",
    "}\n",
    "endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=epc_name)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c1d71",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Get Predictions </h2>\n",
    "\n",
    "Now that our Serverless endpoint is deployed, we can send it text to get predictions from our GPT-2 model. You can use the SageMaker Python SDK or the SageMaker Runtime API to invoke the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "604974eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\\'generated_text\\': \\'\"Working with SageMaker makes machine learning \"a lot easier\" than it used to be.\\\\n\\'}]'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "invoke_client = boto3.client('sagemaker-runtime')\n",
    "prompt = \"Working with SageMaker makes machine learning \"\n",
    "    \n",
    "response = invoke_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                            Body=json.dumps(prompt),\n",
    "                            ContentType='text/csv')\n",
    "\n",
    "response['Body'].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d0ff00",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Monitor Serverless GPT-2 model endpoint </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42056ab7",
   "metadata": {},
   "source": [
    "The `ModelSetupTime` metric helps you to track the time (cold start time) it takes to launch new compute resources to setup Serverless endpoint. It depends on size of the model and container start up time.\n",
    "\n",
    "Serverless endpoint takes around 12 secs to host the GPT-2 model with available compute resources and takes around 3.9 secs to serve the first inference request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dc8eaf",
   "metadata": {},
   "source": [
    "<img src = \"img/se_first_invocation.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e9d6f",
   "metadata": {},
   "source": [
    "Serverless GPT-2 model endpoint is serving subsequent inference requests within 1 sec which is great news üôå."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b99ffc",
   "metadata": {},
   "source": [
    "<img src = \"img/se_second_invocation.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad148b4",
   "metadata": {},
   "source": [
    "Serverless endpoint utilizes 16.14% of the memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10130dd3",
   "metadata": {},
   "source": [
    "<img src = \"img/se_memory_utilization.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec77940",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Clean-up </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70e43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_model(ModelName=model_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=epc_name)\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67b6e4",
   "metadata": {},
   "source": [
    "<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Conclusion </h2>\n",
    "\n",
    "We successfully deployed GPT-2 (text generation model) to Amazon SageMaker Serverless endpoint using the SageMaker boto3 API.\n",
    "\n",
    "The big advantage of Serverless endpoint is that your Data Science team is focusing on the model building process and not spending thousands of dollars while implementing a POC or at the start of a new Product. After the POC is successful, you can easily deploy your model to real-time endpoints with GPUs to handle production workload."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
