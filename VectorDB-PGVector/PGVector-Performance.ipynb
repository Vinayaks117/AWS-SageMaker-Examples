{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to handle a Million Embedding Vectors in the RAG Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Yelp Reviews Dataset\n",
    "\n",
    "This is an open dataset released by [Yelp](https://www.yelp.com/dataset) for learning purposes. It consists of millions of user reviews, business attributes, and over 200,000 pictures from multiple metropolitan areas. This is a very commonly used dataset for NLP challenges globally.\n",
    "\n",
    "Size: 2.66 GB JSON\n",
    "Number of Records: 5,200,000 reviews, 174,000 business attributes, 200,000 pictures, and 11 metropolitan areas\n",
    "\n",
    "We will consider only 1 Million reviews from `yelp_academic_dataset_review.json` that contains full review text data including the `user_id` that wrote the review and the `business_id` the review is written for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Preparation\n",
    "\n",
    "Let’s create the different buckets of data based on the number of reviews like 10k, 50k, 100k, 200k, 400k, 600k, 800k, and 1Million by adjusting the num_records parameter in the following code.\n",
    "\n",
    "We will also clean the review text data by removing handles, hashtags, URLs, special characters, etc.\n",
    "\n",
    "**Note:** In the interest of space utilization on GitHub, I've stored the 10K dataset but you can download the original Yelp review dataset and create different data buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vinayak\n",
      "[nltk_data]     Shanawad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Vinayak\n",
      "[nltk_data]     Shanawad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\Vinayak Shanawad\\AppData\\Local\\Temp\\ipykernel_25080\\7396658.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sliced['cleaned_text'] = df_sliced['text'].apply(clean_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New JSON file created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import json\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub('(\\s)@\\w+|^@\\w+', '', text) # removing handles from reviews\n",
    "    text = re.sub(\"#[A-Za-z0-9_]+\",\"\", text) # removing hastags\n",
    "    text = re.sub(r\"http\\S+\", '', text) #remove URLs\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "                .encode('ascii', 'ignore')\n",
    "                .decode('utf-8', 'ignore'))\n",
    "    text = re.sub(\"'\", \"\", text) #remove single quotes\n",
    "    text = re.sub('\"', \"\", text) #remove double quotes\n",
    "    text = re.sub(\"[^a-zA-Z0-9 \\.]\", \"\", text) #\n",
    "    text = re.sub(r'\\.+', \".\", text) #replace multiple full stops\n",
    "    text = re.sub(' +', ' ', text) #remove more than one space\n",
    "    text.lstrip('0123456789.- ').rstrip('0123456789.- ')\n",
    "    text = text.strip('0123456789.- ') #remove numbers at the beginning of a review\n",
    "    text = text.strip() #remove empty spaces from left and right\n",
    "    return text\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    file_path = \"C://AI-ML-Projects//VectorDB-PGVector//data//yelp//yelp_dataset//yelp_academic_dataset_review.json\"\n",
    "\n",
    "    # Define the columns you want to select\n",
    "    columns = [\"review_id\", \"user_id\", \"text\"]\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    dfs = []\n",
    "\n",
    "    # Open the JSON file and read it line by line\n",
    "    with open(file_path, \"r\") as file:\n",
    "        chunk_size = 100000  # Adjust the chunk size as needed\n",
    "        lines = []\n",
    "        for line in file:\n",
    "            lines.append(line)\n",
    "            # Process the chunk if it reaches the desired size\n",
    "            if len(lines) >= chunk_size:\n",
    "                # Parse each line as JSON\n",
    "                json_objects = [json.loads(line) for line in lines]\n",
    "                # Convert JSON objects to DataFrame\n",
    "                df_chunk = pd.DataFrame(json_objects)\n",
    "                # Select the desired columns\n",
    "                df_chunk = df_chunk[columns]\n",
    "                # Remove double quotes from selected columns\n",
    "                for col in columns:\n",
    "                    df_chunk[col] = df_chunk[col].str.replace('\"', '')\n",
    "                # Append the chunk to the list of DataFrames\n",
    "                dfs.append(df_chunk)\n",
    "                # Clear the list for the next chunk\n",
    "                lines = []\n",
    "\n",
    "        # Process the remaining lines if any\n",
    "        if lines:\n",
    "            json_objects = [json.loads(line) for line in lines]\n",
    "            df_chunk = pd.DataFrame(json_objects)\n",
    "            df_chunk = df_chunk[columns]\n",
    "            for col in columns:\n",
    "                df_chunk[col] = df_chunk[col].str.replace('\"', '')\n",
    "            dfs.append(df_chunk)\n",
    "\n",
    "    # Concatenate the list of DataFrames into a single DataFrame\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    # Define the number of records you want to store in the output file\n",
    "    num_records = 1000000  # Change this to the desired number of records\n",
    "\n",
    "    # Slice the DataFrame to select the specified number of records\n",
    "    df_sliced = df.head(num_records)\n",
    "\n",
    "    df_sliced['cleaned_text'] = df_sliced['text'].apply(clean_text)\n",
    "\n",
    "    # Write the sliced DataFrame to a new JSON file\n",
    "    new_data = df_sliced.to_dict(orient='records')\n",
    "    with open('C://AI-ML-Projects//VectorDB-PGVector//data//yelp//yelp_dataset//review_data_1M.json', 'w') as f:\n",
    "        json.dump(new_data, f, indent=4)\n",
    "\n",
    "    print(\"New JSON file created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Ingestion\n",
    "\n",
    "Let’s create a CustomPGVector class so that we can allow users to store embeddings, and query embeddings into a table by providing the custom embedding size, and retrieve top k relevant documents from a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import torch\n",
    "from typing import Any, Iterable, List, Optional\n",
    "import logging\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_batch\n",
    "\n",
    "class CustomPGVector():\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        connection_string: str,\n",
    "        table_name: str,\n",
    "        embed_dim: int,\n",
    "        logger: Optional[logging.Logger] = None,\n",
    "    ) -> None:\n",
    "        self.table_name = table_name\n",
    "        self.model = model\n",
    "        self.embed_dim = embed_dim\n",
    "        self.connection_string = connection_string\n",
    "        self.logger = logger or logging.getLogger(__name__)\n",
    "        self.__post_init__()\n",
    "\n",
    "    def __post_init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the store.\n",
    "        \"\"\"\n",
    "        self.conn = psycopg2.connect(self.connection_string)\n",
    "        self.conn.autocommit = True\n",
    "\n",
    "    def add_texts(\n",
    "        self,\n",
    "        review_ids: Iterable[int],\n",
    "        user_ids: Iterable[int],\n",
    "        texts: Iterable[str],\n",
    "        metadatas: Optional[List[dict]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Run more texts through the embeddings and add to the vectorstore.\n",
    "\n",
    "        Args:\n",
    "            texts: Iterable of strings to add to the vectorstore.\n",
    "            kwargs: vectorstore specific parameters\n",
    "\n",
    "        Returns:\n",
    "            List of ids from adding the texts into the vectorstore.\n",
    "        \"\"\"\n",
    "        batch_size = 128\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "            embeddings = self.embed_documents(list(batch_texts))\n",
    "\n",
    "            data = []\n",
    "            for review_id, user_id, document, embedding in zip(review_ids, user_ids, batch_texts, embeddings):\n",
    "                doc_id = str(uuid.uuid4())\n",
    "                data.append((doc_id, review_id, user_id, document, embedding))\n",
    "\n",
    "            # Batch insert using execute_batch\n",
    "            query = f'INSERT INTO {self.table_name} (id, user_id, review_id, review, embedding) VALUES (%s, %s, %s, %s, %s) RETURNING id'\n",
    "            with self.conn.cursor() as cur:\n",
    "                execute_batch(cur, query, data)\n",
    "                uuids = [row[0] for row in cur.fetchall()]\n",
    "\n",
    "        return uuids\n",
    "    \n",
    "    def embed_documents(self, texts: Iterable[str]):\n",
    "        \"\"\"Generate embeddings for text documents.\n",
    "\n",
    "        Args:\n",
    "            texts: Iterable of strings to add to the vectorstore.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings adding into the vectorstore.\n",
    "        \"\"\"\n",
    "        batch_size = 64\n",
    "        embeddings = []\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.model.encode(batch_texts, show_progress_bar=True)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "\n",
    "        embeddings_list = [emb.tolist() for emb in embeddings]\n",
    "        return embeddings_list\n",
    "    \n",
    "    \n",
    "    def retrieve_top_k_relevant_docs(self, query_embedding: List[float], k: int):\n",
    "        \"\"\"Retrieve top k relevant documents.\n",
    "        We order by embedding <=> query_embedding which will order by nearest neighbor score.\n",
    "        The <=> operator computes the Cosine distance between two vectors.\n",
    "\n",
    "        Args:\n",
    "            k: Number of relevant documents to return.\n",
    "\n",
    "        Returns:\n",
    "            List of top k relevant documents from the vectorstore.\n",
    "        \"\"\" \n",
    "        try:\n",
    "            with self.conn.cursor() as cur:\n",
    "                #  Cosine distance\n",
    "                # cur.execute('SET ivfflat.probes = 31') # Improve IVF Recall on 1Mrecords\n",
    "                cur.execute(f'SELECT id, review FROM {self.table_name} ORDER BY embedding <=> %s LIMIT {k};', (str(query_embedding),))\n",
    "                # cur.execute(f'EXPLAIN ANALYZE SELECT id, review FROM {self.table_name} ORDER BY embedding <=> %s ASC LIMIT {k};', (str(query_embedding),))\n",
    "                docs = cur.fetchall()\n",
    "        except Exception as e:\n",
    "            self.logger.exception(e)\n",
    "            docs = []\n",
    "\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s store the 1 Million text embeddings into a table in PGVector and provide the required embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Example usage\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "CONNECTION_STRING = 'postgresql://postgres:test@localhost:5432/vector_db'\n",
    "TABLE_NAME = \"REVIEW\"\n",
    "\n",
    "store = CustomPGVector(\n",
    "    model=model,\n",
    "    connection_string=CONNECTION_STRING,\n",
    "    table_name=TABLE_NAME,\n",
    "    embed_dim=768\n",
    "    # table_columns=TABLE_COLUMNS\n",
    ")\n",
    "\n",
    "# Load JSON data into a Pandas DataFrame\n",
    "with open('C://AI-ML-Projects//VectorDB-PGVector//data//yelp//yelp_dataset//review_data_1M.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "review_ids = df.review_id.to_list()\n",
    "user_ids = df.user_id.to_list()\n",
    "texts = df.cleaned_text.to_list()\n",
    "\n",
    "start_time = time.time()\n",
    "uuids = store.add_texts(review_ids, user_ids, texts)\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Retriever - Retrieve top K relevant docs\n",
    "\n",
    "Let's retrieve the top K highest-scoring documents from PGVector:\n",
    "\n",
    "The key functionality:\n",
    "\n",
    "- We select the id and review from the REVIEW table.\n",
    "- The <=> operator computes the cosine distance between two vectors. For cosine distance, the result ranges from 0 (indicating perfect similarity) to 2 (indicating complete dissimilarity).\n",
    "- We order by embedding <=> query_embedding which will order by nearest neighbor score.\n",
    "- Limit to k=10 results to get the top 10 closest matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the query embeddings\n",
    "query = \"In need of a pet-friendly cafe with vegan options and free Wi-Fi.\"\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "query_embeddings = model.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.018878698348999023 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('7f9f5a36-f3b0-4823-b78e-79da08e1bd11',\n",
       "  'Great place Tons of breakfast options including vegan options. Patio is dog friendly'),\n",
       " ('e23cf90e-a43c-4cad-806d-bdb6d2f7f754',\n",
       "  'Amazing cafe. Great for vegans. Wonderful interior. Good music. Nice seating. And fantastic food'),\n",
       " ('8096b5b3-3cbb-46df-9efe-b280a2e79e96',\n",
       "  'Dog friendlyLots of organic healthy choices. Fair trade coffee and lots of yummy juice optionsThe workers are super friendlyNOGMO found here sign Very clean placeAll of their ingredients are organic and stuff I buy at home PerfectoThe barista accidentally made me a hot coffee instead of iced so he is kindly making it over and offered the other one at no charge to anyone in the restaurant.We will definitely be back in the morning for Java and maybe back this afternoon for smoothies or iced tea The lunch menu looks amazing. Veggie burger. Hot cheeses Black bean dip. Hummus. Albacore tuna salad. Grilled cheese sandwich with creamy tomato soup. Sprouts. Avocados. Oh my Organic yogurt parfait. Organic bagel. Chicken salad. Quesadillas'),\n",
       " ('9a31d10f-37a4-49de-9875-9c21f5e5f1d4',\n",
       "  'Lots of vegetarian options outdoor seating cool outdoor patio in the back dog friendly homemade vegan treats'),\n",
       " ('6a261974-b2b0-459d-9fb0-fb0d8f09d260',\n",
       "  'Cute spot with a good variety of food on their menu including some vegan options'),\n",
       " ('5c9c35e6-b906-45e4-8511-6eb3efa631c0',\n",
       "  'Dont believe the hype this location is NOT pet friendly. Employees were incredibly rude about having a dog. The second we walked in everyone behind the counter gave us the nastiest look and immediately the atmosphere was terrible. Prices are incredibly high 8 for a piece of toast with avocado. No wonder everyone hates millennials. I ordered a hot tea it took a ridiculously long time to come out didnt even put a lid on it Also it was 5 for a 10 oz cup. Terrible place would NOT recommend to anyone who has common sense doesnt plan on blowing 30 on coffee'),\n",
       " ('9f449812-e899-4357-9177-443d10077c11',\n",
       "  'My vegan friend and I came across this cute cafe in our hunt for a lunch spot. My Americano was great but the highlight was definitely the vegan cookie that was dessert. It was chewy and delicious. My mozzarella and basil panini was tasty but not amazing I would have preferred a more flavorful spread. The ambiance is eclectic and artsy. Perfect for a leisurelypaced lunch'),\n",
       " ('284b2c21-dcac-4e90-8073-e752aa91e02b',\n",
       "  'Love this vegetarian cafe. Great area with cool setting and clean fast service.Also great prices'),\n",
       " ('2f47314b-61e2-4d7b-867b-62fb017045d6',\n",
       "  'This cafe is great. We are from out of town and we stopped in to ask the employees about vegan options. Immediately we had a support team of 3 people searching for vegan options around the city. We ended up eating here as they do have plenty of vegan options and they offered to cook a meal to fit exactly what we wanted. It was inexpensive came out pretty much instantly and was delicious'),\n",
       " ('7b5a2a11-69f0-466e-be67-7de59223803a',\n",
       "  'This cafe is in a cute quant area and the interior is very large with plenty of seating. They dont have very many outlets so thats something to be aware of. I used xfinity wifi while I was there and that worked fine Their menu isnt very expansive or but they do have a few vegan options. I didnt particularly find anything about it affordable. Their coffee was okay honestly. Not the biggest fan of dark roast but thats what their iced coffee tasted like. They provide oat milk for nondairy options. Lots of people doing work there and theyre pretty conducive to large groups studying together')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the top K highest-scoring documents from PGVector:\n",
    "# %%time\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "result = store.retrieve_top_k_relevant_docs(query_embeddings, 10)\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Recommendations\n",
    "\n",
    "- As discussed in my [Medium article](https://medium.com/@vinayakshanawad/how-to-handle-a-million-embedding-vectors-in-the-rag-application-d10b875a0218), please feel free to add different indexing methods and verify the results with respect to your requirements.\n",
    "- Use `EXPLAIN ANALYZE` to debug performance and make sure we are querying the results using the applied index method.\n",
    "- If you notice in this notebook, I've created a dataset with 1 million reviews and stored the respective embeddings into `REVIEW` table. We can create the different buckets of data based on the number of reviews like 10k, 50k, 100k, 200k, 400k, 600k, 800k, and 1Million. Finally, calculate the performance benchmarking results by running a test on different data buckets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "narrative",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
